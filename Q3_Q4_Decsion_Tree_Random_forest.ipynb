{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3e2SZQzYp5tC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN DATA**"
      ],
      "metadata": {
        "id": "A9huVMS6G6cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=np.array([[0,25,2,0,0,0],\n",
        "                  [1,30,2,0,1,0],\n",
        "                  [2,35,1,0,0,1],\n",
        "                  [3,40,0,0,0,1],\n",
        "                  [4,45,0,1,0,1],\n",
        "                  [5,50,0,1,1,0],\n",
        "                  [6,55,1,1,1,1],\n",
        "                  [7,60,2,0,0,0]])"
      ],
      "metadata": {
        "id": "RvR96dyHxCNM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GET THE POTENTIAL SPLIT**"
      ],
      "metadata": {
        "id": "CQfq4AKh3W_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_potential_split(data, indices):\n",
        "    data = data[indices]\n",
        "    data = data[data[:, 1].argsort()]\n",
        "    label_col1 = data[:, 1]  # Age\n",
        "    label_col2 = data[:, 2]  # Income\n",
        "    label_col3 = data[:, 3]  # Student\n",
        "    label_col4 = data[:, 4]  # Credit rating\n",
        "\n",
        "    age_cut = [(label_col1[i] + label_col1[i+1])/2 for i in range(len(label_col1)-1)] if len(label_col1) > 1 else []\n",
        "    income_cut = list(set(label_col2)) if len(label_col2) > 0 else []\n",
        "    student_cut = list(set(label_col3)) if len(label_col3) > 0 else []\n",
        "    credit_rating = list(set(label_col4)) if len(label_col4) > 0 else []\n",
        "\n",
        "    return np.array(age_cut), np.array(income_cut), np.array(student_cut), np.array(credit_rating)\n",
        "\n"
      ],
      "metadata": {
        "id": "30zB_HWNa6iw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GINI INDEX HELPER FN**"
      ],
      "metadata": {
        "id": "GMf8n0PBTiW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini(labels):\n",
        "    \"\"\"Calculate Gini index using ∑ Pmk(1-Pmk).\"\"\"\n",
        "    class_labels, counts = np.unique(labels, return_counts=True)  # Count occurrences of each class\n",
        "    probabilities = counts / counts.sum()  # Compute class probabilities\n",
        "\n",
        "    gini = np.sum(probabilities * (1 - probabilities))  # ∑ Pmk(1-Pmk)\n",
        "\n",
        "    return gini"
      ],
      "metadata": {
        "id": "9ysf6pSxr-Io"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GINI INDEX**"
      ],
      "metadata": {
        "id": "7MwQnbxbUEn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index(left_labels, right_labels):\n",
        "    \"\"\"Calculate the weighted Gini index for a split.\"\"\"\n",
        "    total_samples = len(left_labels) + len(right_labels)\n",
        "\n",
        "    gini_left =gini(left_labels)\n",
        "    gini_right =gini(right_labels)\n",
        "\n",
        "    weighted_gini = (len(left_labels) / total_samples) * gini_left + (len(right_labels) / total_samples) * gini_right\n",
        "\n",
        "    return weighted_gini"
      ],
      "metadata": {
        "id": "3My4K_jetDT4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINDING THE BEST SPLIT**"
      ],
      "metadata": {
        "id": "TnJgChUlHmKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_split(data, indices, age_splits, income_splits, student_splits, credit_splits):\n",
        "    \"\"\"Finds the best split feature and value that minimizes Gini index.\"\"\"\n",
        "    newdata = data[indices]  # Get only relevant data points\n",
        "\n",
        "    best_gini = float('inf')  #INFINITY intialise\n",
        "    best_split = None  #intally null\n",
        "\n",
        "    # Loop separately for each feature\n",
        "\n",
        "    #  Loop for Age\n",
        "    if len(age_splits)>0:\n",
        "      for threshold in age_splits: # loop on all the thresold point of age split\n",
        "          left_indices = indices[newdata[:, 1] <= threshold]# applying the mask\n",
        "          right_indices = indices[newdata[:, 1] > threshold]\n",
        "\n",
        "          left_labels = data[left_indices][:, -1]\n",
        "          right_labels = data[right_indices][:, -1]\n",
        "\n",
        "          gini = gini_index(left_labels, right_labels)\n",
        "\n",
        "          if gini < best_gini:\n",
        "              best_gini = gini\n",
        "              best_split = (1, threshold, left_indices, right_indices)\n",
        "\n",
        "    #  Loop for Income\n",
        "    if len(income_splits)>0:\n",
        "      for threshold in income_splits:\n",
        "          left_indices = indices[newdata[:, 2] == threshold]\n",
        "          right_indices = indices[newdata[:, 2] != threshold]\n",
        "\n",
        "          left_labels = data[left_indices][:, -1]\n",
        "          right_labels = data[right_indices][:, -1]\n",
        "\n",
        "          gini = gini_index(left_labels, right_labels)\n",
        "\n",
        "          if gini < best_gini:\n",
        "              best_gini = gini\n",
        "              best_split = (2, threshold, left_indices, right_indices)\n",
        "\n",
        "    #  Loop for Student type\n",
        "    if len(student_splits)>0:\n",
        "      for threshold in student_splits:\n",
        "          left_indices = indices[newdata[:, 3] == threshold]\n",
        "          right_indices = indices[newdata[:, 3] != threshold]\n",
        "\n",
        "          left_labels = data[left_indices][:, -1]\n",
        "          right_labels = data[right_indices][:, -1]\n",
        "\n",
        "          gini = gini_index(left_labels, right_labels)\n",
        "\n",
        "          if gini < best_gini:\n",
        "              best_gini = gini\n",
        "              best_split = (3, threshold, left_indices, right_indices)\n",
        "\n",
        "    #  Loop for Credit Rating\n",
        "    if len(credit_splits)>0:\n",
        "      for threshold in credit_splits:\n",
        "          left_indices = indices[newdata[:, 4] == threshold]\n",
        "          right_indices = indices[newdata[:, 4] != threshold]\n",
        "\n",
        "          left_labels = data[left_indices][:, -1]\n",
        "          right_labels = data[right_indices][:, -1]\n",
        "\n",
        "          gini = gini_index(left_labels, right_labels)\n",
        "\n",
        "          if gini < best_gini:\n",
        "              best_gini = gini\n",
        "              best_split = (4, threshold, left_indices, right_indices)\n",
        "\n",
        "\n",
        "\n",
        "    return best_split  # (best feature, best threshold, left indices, right indices)\n"
      ],
      "metadata": {
        "id": "pQGvwMI24rjF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GEETING THE PURITY**"
      ],
      "metadata": {
        "id": "i2F92EqQWH8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def purity(data):\n",
        " label_col=data[:,-1]\n",
        " unique_labels=np.unique(label_col)\n",
        " if(len(unique_labels)==1):\n",
        "  return True\n",
        " else:\n",
        "  return False"
      ],
      "metadata": {
        "id": "hCN71FqBXxZW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATING THE NODE CLASS**"
      ],
      "metadata": {
        "id": "5XUYbiTFWO97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, leaf_indices=None):\n",
        "        self.feature = feature  # Feature used for splitting\n",
        "        self.threshold = threshold  # Threshold value\n",
        "        self.left = left  # Left subtree\n",
        "        self.right = right  # Right subtree\n",
        "        self.leaf_indices = leaf_indices  # Indices of samples in this leaf (if it's a leaf node)\n",
        "\n",
        "    def is_leaf(self):\n",
        "        return self.leaf_indices is not None  # node is a leaf if it has leaf_indices\n"
      ],
      "metadata": {
        "id": "3Jbr0EfVGAah"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATING THE TREE**"
      ],
      "metadata": {
        "id": "SFhSBmbvWcyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth=2\n",
        "def building_the_tree(data, indices, depth=0):\n",
        "    if depth == max_depth or len(indices) <= 2 or purity(data[indices]):\n",
        "        return Node(leaf_indices=list(map(int, indices)))\n",
        "\n",
        "    age_splits, income_splits, student_splits, credit_splits = get_potential_split(data, indices)\n",
        "    best_split = find_best_split(data, indices, age_splits, income_splits, student_splits, credit_splits)\n",
        "\n",
        "    if best_split is None:\n",
        "        return Node(leaf_indices=list(map(int, indices)))\n",
        "\n",
        "    best_feature, best_threshold, left_indices, right_indices = best_split\n",
        "    print(f\"Depth {depth}: Best Split → Feature={best_feature}, Threshold={best_threshold}\")\n",
        "\n",
        "    left_subtree = building_the_tree(data, left_indices, depth+1)\n",
        "    right_subtree = building_the_tree(data, right_indices, depth+1)\n",
        "\n",
        "    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)"
      ],
      "metadata": {
        "id": "uiZoZ0aKcblp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE BULID ON WHOLE TRAINING SAMPLE**"
      ],
      "metadata": {
        "id": "8Nzml3P89x8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILDING THE TREE WITH THE TRAIN DATA**"
      ],
      "metadata": {
        "id": "VeKKXH1KWi6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=building_the_tree(x_train, x_train[:,0], 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5CH-sd_IDyx",
        "outputId": "26eb971d-ef3f-4009-edf8-a08e02da0257"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "Depth 1: Best Split → Feature=1, Threshold=47.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PREDICTING WITH A SAMPLE ON THE BASIS OF ABOVE TREE: this gives leaf node to which sample belong to**"
      ],
      "metadata": {
        "id": "uTEFYYuJW0mS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_leaf_node(node, sample):\n",
        "    \"\"\"Traverses the decision tree to find the leaf node for a given sample.\"\"\"\n",
        "    if node.is_leaf():\n",
        "        return node.leaf_indices  # Return leaf indices if it's a leaf\n",
        "    if node.feature==1:\n",
        "      if sample[node.feature] <= node.threshold:\n",
        "          return predict_leaf_node(node.left, sample)  # Go left\n",
        "      else:\n",
        "          return predict_leaf_node(node.right, sample)  # Go right\n",
        "    else:\n",
        "      if sample[node.feature] == node.threshold:\n",
        "          return predict_leaf_node(node.left, sample)  # Go left\n",
        "      else:\n",
        "          return predict_leaf_node(node.right, sample)  # Go right\n"
      ],
      "metadata": {
        "id": "e8m6gVi9GOuS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> * # THE FEATURES ARE AT INDEX 1,2,3,4\n",
        "> * # INDEX 0 IS FOR INDEX THAT THE SAMPLE HAS IN THE DATASET\n",
        "> * # INDEX 5 IS FOR ITS LABEL\n",
        "> * # THATS WHY PASS -1 AT 0 index AND -1 at last IF NEW SAMPLE COMES"
      ],
      "metadata": {
        "id": "GVpbiv9_AUhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=(predict_leaf_node(n,np.array([-1,42,0,0,1,-1])))\n",
        "print(\"By tree predicted sample would belong to \",i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k4pIdbnIcow",
        "outputId": "00749024-43c4-4f69-cd35-04ea8da65dee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By tree predicted sample would belong to  [2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Return the decision of the tree**"
      ],
      "metadata": {
        "id": "2dcQoXleXsJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def result(x_train,i):\n",
        "  x_train[i][:,-1]\n",
        "  hashmap={}\n",
        "  for i in x_train[i][:,-1]:\n",
        "      if i not in hashmap:\n",
        "        hashmap[i]=0\n",
        "      hashmap[i]=hashmap[i]+1\n",
        "  maxi=float('-inf')\n",
        "  ans=None\n",
        "  for key, value in hashmap.items():\n",
        "      if(value>maxi):\n",
        "        ans=key\n",
        "  return(key)"
      ],
      "metadata": {
        "id": "Beh_qd8mSp7y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Decision of tree trained on whole dataset\",result(x_train,i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxHKtx3d-NZr",
        "outputId": "db2e02e1-e8b8-4079-c581-37be763827ce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision of tree trained on whole dataset 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Genrating 10 bagged dataset**"
      ],
      "metadata": {
        "id": "8B-xL4tWYJHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "bagged_datasets = []\n",
        "oob_for_each_dataset=[]\n",
        "# Generate 10 bagged datasets\n",
        "for _ in range(10):\n",
        "  #these indices are taken from the original sample data\n",
        "    original_indices=np.array([0,1,2,3,4,5,6,7])\n",
        "    X_sample,sample_indices = resample(x_train,original_indices, n_samples=8, replace=True)  # Bootstrap sampling\n",
        "    # Find Out-of-Bag (OOB) samples\n",
        "    oob_indices = np.setdiff1d(original_indices, sample_indices)\n",
        "    oob_for_each_dataset.append(oob_indices)\n",
        "    for i in range(len(X_sample)):\n",
        "      X_sample[i][0]=i\n",
        "    bagged_datasets.append(X_sample)\n",
        "\n",
        "# Print the bagged datasets\n",
        "for i, dataset in enumerate(bagged_datasets):\n",
        "    print(f\"Bagged Dataset {i+1}:\\n{dataset}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOcga2VdtXE-",
        "outputId": "26673c12-c9c5-41cd-9a17-aab1d491479a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagged Dataset 1:\n",
            "[[ 0 60  2  0  0  0]\n",
            " [ 1 50  0  1  1  0]\n",
            " [ 2 35  1  0  0  1]\n",
            " [ 3 50  0  1  1  0]\n",
            " [ 4 60  2  0  0  0]\n",
            " [ 5 30  2  0  1  0]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 45  0  1  0  1]]\n",
            "\n",
            "Bagged Dataset 2:\n",
            "[[ 0 55  1  1  1  1]\n",
            " [ 1 25  2  0  0  0]\n",
            " [ 2 40  0  0  0  1]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 30  2  0  1  0]\n",
            " [ 5 50  0  1  1  0]\n",
            " [ 6 35  1  0  0  1]\n",
            " [ 7 45  0  1  0  1]]\n",
            "\n",
            "Bagged Dataset 3:\n",
            "[[ 0 30  2  0  1  0]\n",
            " [ 1 25  2  0  0  0]\n",
            " [ 2 45  0  1  0  1]\n",
            " [ 3 50  0  1  1  0]\n",
            " [ 4 45  0  1  0  1]\n",
            " [ 5 45  0  1  0  1]\n",
            " [ 6 30  2  0  1  0]\n",
            " [ 7 45  0  1  0  1]]\n",
            "\n",
            "Bagged Dataset 4:\n",
            "[[ 0 30  2  0  1  0]\n",
            " [ 1 50  0  1  1  0]\n",
            " [ 2 30  2  0  1  0]\n",
            " [ 3 30  2  0  1  0]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 45  0  1  0  1]\n",
            " [ 6 50  0  1  1  0]\n",
            " [ 7 40  0  0  0  1]]\n",
            "\n",
            "Bagged Dataset 5:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 40  0  0  0  1]\n",
            " [ 2 25  2  0  0  0]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 50  0  1  1  0]\n",
            " [ 6 60  2  0  0  0]\n",
            " [ 7 35  1  0  0  1]]\n",
            "\n",
            "Bagged Dataset 6:\n",
            "[[ 0 30  2  0  1  0]\n",
            " [ 1 30  2  0  1  0]\n",
            " [ 2 60  2  0  0  0]\n",
            " [ 3 55  1  1  1  1]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 55  1  1  1  1]\n",
            " [ 6 55  1  1  1  1]\n",
            " [ 7 50  0  1  1  0]]\n",
            "\n",
            "Bagged Dataset 7:\n",
            "[[ 0 50  0  1  1  0]\n",
            " [ 1 25  2  0  0  0]\n",
            " [ 2 40  0  0  0  1]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 40  0  0  0  1]\n",
            " [ 5 35  1  0  0  1]\n",
            " [ 6 25  2  0  0  0]\n",
            " [ 7 35  1  0  0  1]]\n",
            "\n",
            "Bagged Dataset 8:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 25  2  0  0  0]\n",
            " [ 2 55  1  1  1  1]\n",
            " [ 3 55  1  1  1  1]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 50  0  1  1  0]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 55  1  1  1  1]]\n",
            "\n",
            "Bagged Dataset 9:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 55  1  1  1  1]\n",
            " [ 2 30  2  0  1  0]\n",
            " [ 3 35  1  0  0  1]\n",
            " [ 4 30  2  0  1  0]\n",
            " [ 5 30  2  0  1  0]\n",
            " [ 6 50  0  1  1  0]\n",
            " [ 7 30  2  0  1  0]]\n",
            "\n",
            "Bagged Dataset 10:\n",
            "[[ 0 50  0  1  1  0]\n",
            " [ 1 45  0  1  0  1]\n",
            " [ 2 60  2  0  0  0]\n",
            " [ 3 50  0  1  1  0]\n",
            " [ 4 60  2  0  0  0]\n",
            " [ 5 50  0  1  1  0]\n",
            " [ 6 45  0  1  0  1]\n",
            " [ 7 60  2  0  0  0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oob_for_each_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wci9OjeulKF6",
        "outputId": "9ce5e786-d222-45d7-fc30-10467488ea06"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([0, 6]),\n",
              " array([7]),\n",
              " array([2, 3, 6, 7]),\n",
              " array([2, 6, 7]),\n",
              " array([1, 6]),\n",
              " array([2, 3, 4]),\n",
              " array([1, 4, 6, 7]),\n",
              " array([1, 2, 7]),\n",
              " array([0, 3, 7]),\n",
              " array([0, 1, 2, 3, 6])]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(prediction):\n",
        "\n",
        "  count_dict = {}\n",
        "  for pred in predictions:\n",
        "      count_dict[pred] = count_dict.get(pred, 0) + 1\n",
        "\n",
        "\n",
        "  final_prediction = max(count_dict, key=count_dict.get)\n",
        "\n",
        "  return(final_prediction)\n"
      ],
      "metadata": {
        "id": "nlnF7auI3ke_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# List to hold the 10 trees\n",
        "nodes = []\n",
        "# List to collect predictions from each tree for the fixed sample/to be predicted sample\n",
        "predictions = []\n",
        "\n",
        "oob_error=[]\n",
        "\n",
        "fixed_sample = np.array([-1, 42, 0, 0, 1, -1])  # fixed sample for prediction\n",
        "\n",
        "# Build trees using the 10 bagged datasets\n",
        "for i in range(10):\n",
        "    tree = building_the_tree(bagged_datasets[i], bagged_datasets[i][:, 0], 0)\n",
        "    nodes.append(tree)\n",
        "\n",
        "# Get the prediction from each tree for the fixed sample\n",
        "for i in range(10):\n",
        "    # Get the leaf node prediction for the fixed sample from the i-th tree\n",
        "    leaf_pred = predict_leaf_node(nodes[i], fixed_sample)\n",
        "    # Convert the leaf node prediction into a class label/class predicted\n",
        "    class_pred = result(bagged_datasets[i], leaf_pred)\n",
        "    predictions.append(class_pred)\n",
        "    for f in oob_for_each_dataset[i]:\n",
        "      leaf_pred = predict_leaf_node(nodes[i], x_train[f])\n",
        "      class_pred = result(bagged_datasets[i], leaf_pred)\n",
        "      oob_error.append(class_pred-x_train[f][-1])\n",
        "\n",
        "#final result\n",
        "final_oob_error=0\n",
        "count=0\n",
        "for h in oob_error:\n",
        "  final_oob_error=final_oob_error+pow(h,2)\n",
        "  count=count+1\n",
        "final_oob_error=final_oob_error/count\n",
        "print(\"final oob error is:\",final_oob_error)\n",
        "# Final prediction by majority voting\n",
        "final_prediction = counter(predictions)\n",
        "print(\"Final prediction (majority vote) is:\", final_prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js4YjDHAEHMq",
        "outputId": "c86b2b8b-1580-451f-9ab1-fd96471a3c9c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 0: Best Split → Feature=1, Threshold=47.5\n",
            "Depth 1: Best Split → Feature=1, Threshold=32.5\n",
            "Depth 0: Best Split → Feature=1, Threshold=32.5\n",
            "Depth 1: Best Split → Feature=1, Threshold=47.5\n",
            "Depth 0: Best Split → Feature=1, Threshold=30.0\n",
            "Depth 1: Best Split → Feature=1, Threshold=45.0\n",
            "Depth 0: Best Split → Feature=4, Threshold=0\n",
            "Depth 1: Best Split → Feature=1, Threshold=32.5\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "Depth 1: Best Split → Feature=1, Threshold=47.5\n",
            "Depth 0: Best Split → Feature=2, Threshold=1\n",
            "Depth 0: Best Split → Feature=1, Threshold=25.0\n",
            "Depth 1: Best Split → Feature=1, Threshold=40.0\n",
            "Depth 0: Best Split → Feature=1, Threshold=25.0\n",
            "Depth 1: Best Split → Feature=1, Threshold=52.5\n",
            "Depth 0: Best Split → Feature=1, Threshold=30.0\n",
            "Depth 1: Best Split → Feature=1, Threshold=47.5\n",
            "Depth 0: Best Split → Feature=1, Threshold=45.0\n",
            "final oob error is: 0.5\n",
            "Final prediction (majority vote) is: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILDING THE RANDOM FOREST**"
      ],
      "metadata": {
        "id": "3S2iN-04uUIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_depth = 2\n",
        "import random\n",
        "def building_the_random_forest_tree(data, indices, depth):\n",
        "    if depth == max_depth or len(indices) <= 2 or purity(data[indices]):\n",
        "        return Node(leaf_indices=list(map(int, indices)))\n",
        "\n",
        "    age_splits, income_splits, student_splits, credit_splits = get_potential_split(data, indices)\n",
        "\n",
        "    # Corrected: Map features to their actual column indices (1-4)\n",
        "    available_features = [1, 2, 3, 4]  # Age (col1), Income (col2), Student (col3), Credit (col4)\n",
        "    selected_features = random.sample(available_features, 2)  # Now correctly picks 2 actual features\n",
        "    print(f\"selected feature,{selected_features}\")\n",
        "    # Disable non-selected features by emptying their splits\n",
        "    if 1 not in selected_features: age_splits = []\n",
        "    if 2 not in selected_features: income_splits = []\n",
        "    if 3 not in selected_features: student_splits = []\n",
        "    if 4 not in selected_features: credit_splits = []\n",
        "\n",
        "    best_split = find_best_split(data, indices, age_splits, income_splits, student_splits, credit_splits)\n",
        "\n",
        "    if best_split is None:\n",
        "        return Node(leaf_indices=list(map(int, indices)))\n",
        "\n",
        "    best_feature, best_threshold, left_indices, right_indices = best_split\n",
        "    print(f\"Depth {depth}: Best Split → Feature={best_feature}, Threshold={best_threshold}\")\n",
        "\n",
        "    left_subtree = building_the_random_forest_tree(data, left_indices, depth+1)\n",
        "    right_subtree = building_the_random_forest_tree(data, right_indices, depth+1)\n",
        "\n",
        "    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)"
      ],
      "metadata": {
        "id": "FmYEkn3iebFn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Genrating Bagged dataset for random forest**"
      ],
      "metadata": {
        "id": "SUyKTfPtx4dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "bagged_datasets_for_rf = []\n",
        "oob_for_each_dataset_rf=[]\n",
        "\n",
        "# Generate 10 bagged datasets\n",
        "for _ in range(10):\n",
        "  #these indices are taken from the original sample data\n",
        "    original_indices=np.array([0,1,2,3,4,5,6,7])\n",
        "    X_sample,sample_indices = resample(x_train,original_indices, n_samples=8, replace=True)  # Bootstrap sampling\n",
        "    # Find Out-of-Bag (OOB) samples\n",
        "    oob_indices = np.setdiff1d(original_indices, sample_indices)\n",
        "    oob_for_each_dataset_rf.append(oob_indices)\n",
        "    for i in range(len(X_sample)):\n",
        "      X_sample[i][0]=i\n",
        "    bagged_datasets_for_rf.append(X_sample)\n",
        "\n",
        "# Print the bagged datasets\n",
        "for i, dataset in enumerate(bagged_datasets_for_rf):\n",
        "    print(f\"Bagged Dataset {i+1}:\\n{dataset}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAFpaQfhxwWm",
        "outputId": "d36f332e-8b2d-4103-9e12-92cedf3ca436"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagged Dataset 1:\n",
            "[[ 0 25  2  0  0  0]\n",
            " [ 1 25  2  0  0  0]\n",
            " [ 2 45  0  1  0  1]\n",
            " [ 3 45  0  1  0  1]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 35  1  0  0  1]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 35  1  0  0  1]]\n",
            "\n",
            "Bagged Dataset 2:\n",
            "[[ 0 55  1  1  1  1]\n",
            " [ 1 60  2  0  0  0]\n",
            " [ 2 45  0  1  0  1]\n",
            " [ 3 45  0  1  0  1]\n",
            " [ 4 35  1  0  0  1]\n",
            " [ 5 50  0  1  1  0]\n",
            " [ 6 45  0  1  0  1]\n",
            " [ 7 25  2  0  0  0]]\n",
            "\n",
            "Bagged Dataset 3:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 45  0  1  0  1]\n",
            " [ 2 45  0  1  0  1]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 30  2  0  1  0]\n",
            " [ 5 25  2  0  0  0]\n",
            " [ 6 45  0  1  0  1]\n",
            " [ 7 30  2  0  1  0]]\n",
            "\n",
            "Bagged Dataset 4:\n",
            "[[ 0 35  1  0  0  1]\n",
            " [ 1 55  1  1  1  1]\n",
            " [ 2 45  0  1  0  1]\n",
            " [ 3 55  1  1  1  1]\n",
            " [ 4 60  2  0  0  0]\n",
            " [ 5 30  2  0  1  0]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 30  2  0  1  0]]\n",
            "\n",
            "Bagged Dataset 5:\n",
            "[[ 0 50  0  1  1  0]\n",
            " [ 1 50  0  1  1  0]\n",
            " [ 2 30  2  0  1  0]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 40  0  0  0  1]\n",
            " [ 5 60  2  0  0  0]\n",
            " [ 6 60  2  0  0  0]\n",
            " [ 7 50  0  1  1  0]]\n",
            "\n",
            "Bagged Dataset 6:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 40  0  0  0  1]\n",
            " [ 2 30  2  0  1  0]\n",
            " [ 3 45  0  1  0  1]\n",
            " [ 4 40  0  0  0  1]\n",
            " [ 5 45  0  1  0  1]\n",
            " [ 6 35  1  0  0  1]\n",
            " [ 7 45  0  1  0  1]]\n",
            "\n",
            "Bagged Dataset 7:\n",
            "[[ 0 60  2  0  0  0]\n",
            " [ 1 30  2  0  1  0]\n",
            " [ 2 30  2  0  1  0]\n",
            " [ 3 30  2  0  1  0]\n",
            " [ 4 35  1  0  0  1]\n",
            " [ 5 60  2  0  0  0]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 25  2  0  0  0]]\n",
            "\n",
            "Bagged Dataset 8:\n",
            "[[ 0 45  0  1  0  1]\n",
            " [ 1 45  0  1  0  1]\n",
            " [ 2 50  0  1  1  0]\n",
            " [ 3 50  0  1  1  0]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 35  1  0  0  1]\n",
            " [ 6 60  2  0  0  0]\n",
            " [ 7 25  2  0  0  0]]\n",
            "\n",
            "Bagged Dataset 9:\n",
            "[[ 0 25  2  0  0  0]\n",
            " [ 1 45  0  1  0  1]\n",
            " [ 2 40  0  0  0  1]\n",
            " [ 3 35  1  0  0  1]\n",
            " [ 4 55  1  1  1  1]\n",
            " [ 5 40  0  0  0  1]\n",
            " [ 6 40  0  0  0  1]\n",
            " [ 7 25  2  0  0  0]]\n",
            "\n",
            "Bagged Dataset 10:\n",
            "[[ 0 25  2  0  0  0]\n",
            " [ 1 45  0  1  0  1]\n",
            " [ 2 35  1  0  0  1]\n",
            " [ 3 40  0  0  0  1]\n",
            " [ 4 25  2  0  0  0]\n",
            " [ 5 30  2  0  1  0]\n",
            " [ 6 25  2  0  0  0]\n",
            " [ 7 60  2  0  0  0]]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# List to hold the 10 trees\n",
        "nodes_rf = []\n",
        "# List to collect predictions from each tree for the fixed sample\n",
        "predictions_rf = []\n",
        "\n",
        "#oob error store\n",
        "oob_error_rf=[]\n",
        "\n",
        "fixed_sample_rf = np.array([-1, 42, 0, 0, 1, -1])  # fixed sample for prediction\n",
        "\n",
        "# Build trees using the 10 bagged datasets\n",
        "for i in range(10):\n",
        "    tree = building_the_random_forest_tree(bagged_datasets_for_rf[i], bagged_datasets_for_rf[i][:, 0], 0)\n",
        "    nodes_rf.append(tree)\n",
        "\n",
        "# Get the prediction from each tree for the fixed sample\n",
        "for i in range(10):\n",
        "    # Get the leaf node prediction for the fixed sample from the i-th tree\n",
        "    leaf_pred = predict_leaf_node(nodes_rf[i], fixed_sample_rf)\n",
        "    # Convert the leaf node prediction into a class label\n",
        "    class_pred = result(bagged_datasets_for_rf[i], leaf_pred)\n",
        "    predictions_rf.append(class_pred)\n",
        "    for f in oob_for_each_dataset_rf[i]:\n",
        "      leaf_pred = predict_leaf_node(nodes_rf[i], x_train[f])\n",
        "      class_pred = result(bagged_datasets_for_rf[i], leaf_pred)\n",
        "      oob_error_rf.append(class_pred-x_train[f][-1])\n",
        "\n",
        "#final result\n",
        "final_oob_error_rf=0\n",
        "count=0\n",
        "for h in oob_error_rf:\n",
        "  final_oob_error_rf=final_oob_error_rf+pow(h,2)\n",
        "  count=count+1\n",
        "final_oob_error_rf=final_oob_error_rf/count\n",
        "print(\"final oob error is:\",final_oob_error_rf)\n",
        "# Final prediction by majority voting\n",
        "final_prediction_rf = counter(predictions_rf)\n",
        "print(\"Final prediction (majority vote) is:\", final_prediction_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d29efc-d35e-424e-cc8a-e31c5af5c31f",
        "id": "hQOJPHkO6YRP"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "selected feature,[2, 4]\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "selected feature,[4, 2]\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "selected feature,[2, 4]\n",
            "Depth 1: Best Split → Feature=4, Threshold=0\n",
            "selected feature,[1, 3]\n",
            "Depth 0: Best Split → Feature=1, Threshold=30.0\n",
            "selected feature,[1, 2]\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "selected feature,[1, 2]\n",
            "Depth 0: Best Split → Feature=1, Threshold=40.0\n",
            "selected feature,[1, 4]\n",
            "Depth 1: Best Split → Feature=1, Threshold=35.0\n",
            "selected feature,[4, 1]\n",
            "Depth 0: Best Split → Feature=1, Threshold=32.5\n",
            "selected feature,[4, 3]\n",
            "Depth 0: Best Split → Feature=4, Threshold=0\n",
            "selected feature,[1, 4]\n",
            "Depth 1: Best Split → Feature=1, Threshold=50.0\n",
            "selected feature,[2, 3]\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "selected feature,[2, 1]\n",
            "Depth 1: Best Split → Feature=1, Threshold=45.0\n",
            "selected feature,[2, 1]\n",
            "Depth 0: Best Split → Feature=1, Threshold=25.0\n",
            "selected feature,[4, 2]\n",
            "Depth 0: Best Split → Feature=2, Threshold=2\n",
            "final oob error is: 0.5161290322580645\n",
            "Final prediction (majority vote) is: 1\n"
          ]
        }
      ]
    }
  ]
}